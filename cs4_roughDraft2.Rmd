---
title: "Case Study 4"
subtitle: |
  | Web Scraping
  | DS7333
author: "Jason Rupp and Indy Dhillon"
date: "February 4, 2021"
output:
  prettydoc::html_pretty:
    theme: cayman
---

```{css, echo=FALSE}
p.caption {
  font-size: 0.8em;
}
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE)
library(tidyverse)
library(rmarkdown)
library(knitr)
library(kableExtra)
library(gridExtra)
#source("cs4_scripts_rd.R")
```



# Introduction

Web scraping can be a very powerful tool that can be used to gather data from the internet. One form of web scraping involves locating and extracting information embedded in HTML code. Several software packages have been developed to assist with web scraping, this case study will utilize two such packages that are included within the `tidyverse`, `xml2` and `rvest`. Using this technique can be a very valuable asset to a Data Scientist, as this case study will demonstrate, it can automate data collection across a number of web pages, decreasing the amount of time required to gather data for analysis.

Though there are a few means by which web scraping can be accomplished, one such method involved parsing HTML, or Hyper Text Markup Language, text to find data of interest. Data on websites are typically contained within pre-defined HTML tags, a list can be found <a href = "https://www.w3schools.com/tags/default.asp"> here</a>. If there is data of interest on a website, the tag associated with the data can be located in the HTML text, and collected for analysis.

Even though it can be very useful technique, there is a major pitfall associated with web scraping that can interfere with data acquisition. If the site undergoes maintenance, or if the site does a major over hall and makes several changes to the site, it is possible that the tools being used to extract data may not be effective on the changed site. In spite of this draw back, web scraping is a very effective means by which data can be acquired.

The purpose of this Case Study is to utilize software to scrape data from the internet, and perform minor statistical analysis on the data acquired.

# Background

## Case Study Purpose

This Case Study is a slight modification of an exercise found in Chapter 2 of the book "Data Science in R: A Case Studies Approach to Computational Reasoning and Problem Solving" titled *Modeling Runnersâ€™ Times in the Cherry Blossom Race*. The Cherry Blossom Race is an annual race in Washington, DC that dates back to 1973. Run times for each race have been posted online at [this website](http://www.cballtimeresults.org/performances). This chapter in the text book provides tools and detailed explanation outlining the steps of data acquisition and cleaning, however this where the methods in this Case Study and the text book will diverge. 

The instructions provided in the text book were only useful prior until about January 22nd, 2021, as The Cherry Blossom changed the format of the website around this time. With the website in a completely new format, using the tools in the book will no longer be a viable means of data collection.

The aim of this case study is to develop new tools to automate collection of race data from 1999-2012 with particular focus on the Mens division to perform statistical analysis on the data to answer questions of interest. 

## Questions of Interest

There has been an apparent trend in the age of male runners entering the Cherry Blossom Race, male runners in 1999 were typically older than those which ran the race in 2012. This case study will compare the age distribution of the runners across all 14 years of the races by using quantile-quantile plots, boxplots, density curves, and other methods to make comparisons. How have the distributions changed over the years and was it gradual? 

# Methods
The new format for the Cherry Blossom website has the same format for every race, but the new pages are paginated and a different call has to be made for every race, year, and gender. The function below retrieves the table containing the data on each page, which is identified by the <table> html tag. 

```{r return_dataTable}
return_dataTable <- function(url)
{
  # Fetch the page passed as url, expects cherry blossom database site
  # example: http://www.cballtimeresults.org/performances?division=Overall+Women&page=1&section=10M&sex=W&utf8=%E2%9C%93&year=1999
  # Page 1 of Womens 10 Mile from 1999
  page <- xml2::read_html(url)
  
  # create dataframe with results 
  # data needed is in only <table></table> on page, makes it easy
  dataTable <- page %>% # take page
    rvest::html_node("table") %>% # find table
    rvest::html_table() # convert table to dataframe
  
  return(dataTable)
}
```

The next function gathers all the data from the paginated tables for an indivdiual year-gender combo. The URL string are build based on these variables and the pages are queried one at a time using the total number of runner to figure out how many pages to query (since there are 20 runner displayed per page). 

```{r collect_data}
collect_data <- function(year, gender)
{
  # Fetch data for first page of desired gender/year combo, needed to create dataframe to add subsequent pages
  # Column header names created issues with rbind(), got colnames from the site on first page as seed for loop
  # Additionally, this will find out how many pages of results to fetch from the "PiS/TiS" column returned
  url_first <- paste("http://www.cballtimeresults.org/performances?division=Overall+",
                     gender,
                     "&page=",
                     1,
                     "&section=10M&sex=",
                     substring(gender,1,1),
                     "&utf8=%E2%9C%93&year=",
                     year,
                     sep = "")  
  
  # Prints the url of first page to show progress
  print(url_first) 
  
  # Seed dataframe for upcoming loop
  data1 <- return_dataTable(url_first) 
  
  # This column shows Place in Sex/Total in Sex
  n = data1$`PiS/TiS`[1] 
  
  # Split 1st value on / select second element, total runners/gender
  n = as.numeric(strsplit(n,"/")[[1]][2]) 
  
  # Pages display 20 results so this finds the n for pages
  n = trunc(n/20) + 1 
  
  # iterate through pages for the year, adding to seed dataframe per page
  i = 2
  
  for (i in 2:n) {
    url_loop <- paste("http://www.cballtimeresults.org/performances?division=Overall+",
                      gender,
                      "&page=",
                      i,
                      "&section=10M&sex=",
                      substring(gender,1,1),
                      "&utf8=%E2%9C%93&year=",
                      year,
                      sep = "")  
    data2 <- return_dataTable(url_loop)
    print(paste(gender, " - ", i, " - ", year))
    data1 <- bind_rows(mutate_all(data1, as.character), mutate_all(data2, as.character))
  }
    # return entire year df
  return(data1)
}
```

Next is a function that takes the list of years for one gender and retrieves the data for the gender over the year range using the previous "get_all_data" function. It then stores this data as a csv for later processing.

```{r get_all_data}
get_all_data = function(years, gender)
{
  # seed dataframe for loop due to column name issue
  # fetches first year data
  df_all <- collect_data(years[1], gender)
  
  # iterate through the years of single gender passed
  for(i in seq(2, length(years)))
    {
    # add to year of data to total dataframe
    df_all <- bind_rows(
      mutate_all(df_all, as.character), 
      mutate_all(collect_data(years[i],gender), as.character)
      )
  }
  
  # write results to csv with file name "allData[gender].csv" for later analysis
  write_csv(df_all, paste("caseStudy04_Scraping/data/allData", gender,".csv",sep = ""))
  return(df_all)
}
```

Below is the function that puts together everything above to build the list of years and scrape the data for both genders using the "get_all_data" funciton above.

``` {r scrap_data}
scrap_data = function()
  {
  # desired years
  years <- seq(1999,2012,1) 
  # Both genders, need to be in this format will be part of url
  genders <- c("Men", "Women")
  for (i in 1:length(genders)) {
    # update print
    print(paste(genders[i], " - Starting"))
    
    # collect all data for both genders
    get_all_data(years, genders[i])
    
    #update print
    print(paste(genders[i], " - COMPLETE!!!"))
  }
  #update print
  print("DATA SCRAPE COMPLETE!!!")
}
scrap_data()
```


Once all the data has been scraped, we proceed with our analysis to investigate the change in age distribution of men entering the race over time. We start by loading the csv for the Men's data the we
scraped above and adding column names.

``` {r Load men's data}
# column names for data from site
col_names <- c("race", 
               "name", 
               "age", 
               "time", 
               "pace", 
               "placeInSex", 
               "division", 
               "placeInDivision", 
               "hometown")

# read csv of compiled Mens data
df_all_mens <- read_csv("caseStudy04_Scraping/data/allDataMen.csv", 
                        col_types = cols(Age = col_character()))

# set the names for the df
names(df_all_mens) <- col_names

# show how many rows in total dataset
dim(df_all_mens)
```

The data still requires some processing to be useful for our analysis. In the next code block the rows with age values of "NR" are removed. Then the age and year are cast as integers and division as a factor.

``` {r Format types}
# eliminate rows with NR for age
df_trim <- df_all_mens[which(df_all_mens$age != "NR"),]

# show how many eliminated from data
dim(df_trim)

# switch the age column from character to integer
df_trim$age <- as.integer(df_trim$age)

# this will take the race column split on whitespace, and index year data
minus <- which(unlist(strsplit(df_trim$race, "\\s")) != "10M")

# creates year column 
df_trim$year <- as.integer(unlist(strsplit(df_trim$race, "\\s"))[minus])

# turns division column into factor
df_trim$division <- as.factor(df_trim$division)
```


The hometown feature requires a bit more formatting because it can contain a "city, state" combo for US runners or simply a country for international runners. The next code block splits the hometown field by the comma and when the comma is missing, it is presumed that the runner is international.

For this analysis we are not interested at the state level, but we keep the state and international country data and we put both into a field called "state2".

!!!!!! Jason, I'm not sure what I wrote above is accurate !!!!!


``` {r Format state}
extract_state = function(string)
  {
  tmp <- string
  if (grepl(",", string) == "TRUE"){
    tmp <- str_trim(strsplit(string, ",")[[1]][2])
  }
  return(tmp)
}

#extract_state(df_trim$hometown[900])

df_trim$state2 <- unlist(lapply(df_trim$hometown, extract_state))

df_trim$state <- ifelse((grepl(",", df_trim$hometown) == "TRUE"),substr(df_trim$hometown, nchar(df_trim$hometown) - 2, nchar(df_trim$hometown)), "NaS")

df_trim[which(str_locate(df_trim$hometown, ",") - nchar(df_trim$hometown) != -3),]
df_trim[-which(grepl(",", df_trim$hometown) == "TRUE" | nchar(df_trim$hometown) <= 3),]

unlist(unique(df_trim[-which(grepl(",", df_trim$hometown) == "TRUE" | nchar(df_trim$hometown) <= 3),"hometown"]))
```


With the variables of interest extracted, we are create summaries of the age by year and the count of runners per division per year. We also add 0 runners for two divisions with missing data to make further analysis easier.

``` {r Summary dataframes}
# create summary dataframe by year
df_age_stats <- df_trim %>%
                  group_by(year) %>%
                  summarize(
                    mean = mean(age), 
                    sd = sd(age),
                    oldest = max(age),
                    yngest = min(age),
                    n=n(),
                    skew = skewness(age)
                    )

# create dataframe of number of racers per division, per year
df_div_stats <- df_trim %>%
                  group_by(division, year) %>%
                  summarize(
                    n=n()
                    )

# names for above dataframe, need to add data
div_stat_names <- c("division", "year", "n")

# adding missing data
df_row_to_add <- data.frame("M8099", 2002, 0)
df_row_to_add2 <- data.frame("M8099", 2000, 0)
names(df_row_to_add)<-div_stat_names
names(df_row_to_add2)<-div_stat_names

df_div_stats <- rbind(df_div_stats, df_row_to_add)
df_div_stats <- rbind(df_div_stats, df_row_to_add2)
```

!!!!!!! This is where it errors for me !!!!!!!!!

``` {r}
df_div_stats$division <- as.factor(df_div_stats$division)
```

## Data Cleanup

## Finding k-Nearest Neighbors




## Cross Validation


## Final Model Selection



# Results

## Cross Validation



## Evaluating the options of $k$



## Evaluating MACs



## Evaluating Neighbor Strategy






## Final Model Design






# Conclusion



# Appendix

## Sources

[w3schools HTML Tags: https://www.w3schools.com/tags/default.asp](https://www.w3schools.com/tags/default.asp)

**Nolan, Deborah; Lang, Duncan Temple.**; *Data Science in R: A Case Studies Approach to Computational Reasoning and Problem Solving*; Chapter 2 pgs 45-103.

[Cherry Blossom Race](https://www.cherryblossom.org/)

[Cherry Blossom Race Database](http://www.cballtimeresults.org/performances)

## Functions

